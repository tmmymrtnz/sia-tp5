# TP 5 â€“ SIA Â· Deep Learning  
*Autoencoders, Denoising AE y Variational AE*

> **Sistemas de Inteligencia Artificial**  
> ITBA â€“ 2025  

[ğŸ“‘ Enunciado oficial](docs/sia-tp5.pdf)

---

## IntroducciÃ³n

Este TP explora tres variantes de autoencoders sobre distintos datasets:

* **AE bÃ¡sico** â€“ aprende los 32 caracteres 5 Ã— 7 de `font.h`  
* **Denoising AE** â€“ quita ruido binario de los mismos caracteres  
* **VAE** â€“ genera emojis (OpenMoji) y disentangled sprites (dSprites)

El cÃ³digo es *NumPy-only* y corre en CPU en minutos.

---

## âš™ï¸ InstalaciÃ³n rÃ¡pida

1. Clonar el repositorio  
   ```git clone https://github.com/tu-usuario/sia-tp5.git```
2. Crear y activar entorno virtual  
   ```python -m venv venv && source venv/bin/activate```
3. Instalar dependencias  
   ```pip install -r requirements.txt```

---

## â–¶ï¸ EjecuciÃ³n

### Ejercicio 1a â€” Autoencoder bÃ¡sico (font.h)
1. Para encontrar la arquitectura Ã³ptima, se debe correr:
   ```bash
   python src/ex1/part_a/ej_b.py configs/ex1/config_a_1.json
   ```
   Donde el archivo de configuraciÃ³n contiene distintos experimentos y el directorio de font.h
   En el directorio `/experimentos` se guardarÃ¡ el grÃ¡fico de pÃ©rdida por Ã©poca y un archivo json con los resultados finales de cada experimento.

2. Para graficar el espacio latente, se debe correr:
   ```bash 
   python src/ex1/part_a/ej_c.py
   ```
   TomarÃ¡ directamente el archivo font.h y la configuraciÃ³n `configs/ex1/optimal.json` donde se encuentran los detalles de la arquitectura Ã³ptima hallada en el item anterior.
   Genera un grÃ¡fico del espacio latente 2D con los caracteres proyectados sobre el punto (z) que los codifica.

3. Para generar nuevas letras que no pertenecen al conjunto de entrenamiento, se debe correr:
   ```bash
   python src/ex1/part_a/ej_d.py
   ```
   Entrena al autoencoder con las configuraciones Ã³ptimas de `configs/ex1/optimal.json`, luego se realizan tres grÃ¡ficos: primero se divide el espacio latente en 25 bloques (5x5), cada uno con un punto central (x,y). Se usa este punto (x,y) como entrada del decodificador, generando un caracter para este punto. Se genera un grÃ¡fico con los 25 caracteres generados. Se repite el proceso para 10x10 y 20x20.

---

### Ejercicio 1b â€” Denoising Autoencoder
Para distorsionar las entradas y estudiar la capacidad del Autoencoder de eliminar el ruido, se debe correr:
```bash
python src/analisis_dae.py configs/ex1/optimal.json data/font.h
```
En la configuraciÃ³n se especifica la arquitectura Ã³ptima y el nivel de ruido para la prueba.
Se harÃ¡n 5 ejecuciones de esta prueba de denoising.
Se generarÃ¡n tres archivos en `/checkpoints`
* `dae_loss_avg_plot.png` con la pÃ©rdida promedio a lo largo de las Ã©pocas en las 5 ejecuciones
* `ejemplo_mejor.png` y `ejemplo_peor.png` con una visualizaciÃ³n de la entrada con ruido, la reconstrucciÃ³n del AE y la entrada original para el caracter con menos bits de error en promedio, y el caracter con mÃ¡s bits de error en promedio.

---

### Ejercicio 2 â€” Variational AE

Runner  `src/runner_vae.py`  
**Flags clave**  
â€¢ `--dataset`â€ƒfaces_only | emoji_full | dsprites | faces_lfw  
â€¢ `--img_size`â€ƒ28 / 32 / 64 (se ajusta solo para dsprites / ffhq / lfw)  
â€¢ `--beta_final`â€ƒvalor final de Î²  
â€¢ `--kl_ramp_epochs`â€ƒÃ©pocas para annealing 0 â†’ Î²  
â€¢ `--max_emojis`â€ƒmÃ¡x. de imÃ¡genes a usar  
â€¢ `--refresh_dataset`â€ƒre-descarga forzada (OpenMoji)

**Comandos sugeridos**

| Dataset | Comando | Comentario |
|---------|---------|------------|
| Caras amarillas (197Ã—32 px) | ```python src/runner_vae.py configs/vae_optimized.json --dataset faces_only``` | demo rÃ¡pida |
| OpenMoji completo (â‰ˆ 4 200) | ```python src/runner_vae.py configs/vae_optimized.json --dataset emoji_full --max_emojis 2000``` | mÃ¡s diversidad |
| dSprites 64Ã—64 | ```python src/runner_vae.py configs/vae_optimized.json --dataset dsprites --beta_final 4 --kl_ramp_epochs 100``` | prueba disentanglement |
| LFW deepfunneled (8 000) | ```python src/runner_vae.py configs/vae_optimized.json --dataset faces_lfw --max_emojis 8000 --img_size 64``` | descarga via TF-Datasets |

Cada ejecuciÃ³n crea  
`checkpoints/<dataset>_img<size>_beta<Î²>_<timestamp>/`  
â€ƒÂ· vae_weights.npzâ€ƒÂ· loss_history.npyâ€ƒÂ· vae_generated.png  

AdemÃ¡s se guarda un *sample* del dataset en  
`data/sample_<dataset>.png`

## ğŸ“ Estructura de carpetas

src/              Â· implementaciÃ³n AE / VAE / trainer  
configs/          Â· JSON de hiper-parÃ¡metros  
data/             Â· datasets y samples de referencia  
checkpoints/      Â· pesos + salidas organizadas por experimento  
docs/             Â· enunciado y apuntes Ãºtiles  

---

## ğŸ“Š AnÃ¡lisis de resultados  *(pendiente)*

Se agregarÃ¡n:  
â€¢ grÃ¡ficas de convergencia,  
â€¢ proyecciÃ³n 2-D del espacio latente (AE) y  
â€¢ interpolaciones en el VAE con distintas Î².