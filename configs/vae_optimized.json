{
    "encoder": {
      "layer_sizes": [1024, 512, 256, 64, 32],
      "activations": ["relu", "relu", "relu", "relu", "identity"]
    },
    "decoder": {
      "layer_sizes": [16, 64, 256, 512, 1024],
      "activations": ["relu", "relu", "relu", "relu", "sigmoid"]
    },
    "latent_dim": 16,
    "dropout_rate": 0.0,
    "reconstruction_loss": "bce",
    "optimizer": "adam",
    "optim_kwargs": { "learning_rate": 0.0003 },
    "batch_size": 64,
    "max_epochs": 300,
    "log_every": 20,
    "patience": 40,
    "min_delta": 1e-4,
    "kl_ramp_epochs": 50
  }
  